{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71769ee8-ec98-4f11-8fde-14d724f2c087",
   "metadata": {},
   "source": [
    "# Dealing with variable-length sequences efficiently\n",
    "\n",
    "This notebook is meant to be run using a GPU.\n",
    "\n",
    "RNNs can work with variable length sequences. This notebook shows how to do this efficiently with `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba771a15-3a91-4137-afe4-f8887850629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3547d20-fcde-4aa8-87d2-b0ca7c273240",
   "metadata": {},
   "source": [
    "Let's generate some toy data - consisting of sequences around length 100 with 3 features each (can think of this as e.g. particle tracks with momentum components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281125f-c316-409d-814c-b0d5de29d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.random.poisson(100, size=100000)\n",
    "arrays = [np.random.normal(size=(count, 3)) for count in counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a44512-f8fd-477a-8274-8c57d049a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08765580-b05c-4ed2-9353-1085890acb2a",
   "metadata": {},
   "source": [
    "We want to feed this through the following 2-layer LSTM model - it takes a batch of sequences of arbitrary length and outputs a batch of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f763b51-ce94-44a6-ab44-cde6cf488183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, input_shape=(None, 3), return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c77d6d-b9e5-4b33-b42b-c03be56053ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd65c3-748e-4c70-82a6-0824955a6082",
   "metadata": {},
   "source": [
    "How do we feed in the variable length sequences? Well, since the first two input dimensions of our model are unspecified `(batch_size, sequence_length)` we can pass each sequence separately. Let's see how fast this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b2d60-1bc9-4e7e-9899-6240ce9e124f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for array in tqdm(arrays):\n",
    "    model(array[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef1d32-fce7-4380-9eb9-a5075723f6d1",
   "metadata": {},
   "source": [
    "Doesn't seem that bad does it?\n",
    "\n",
    "Wait! We haven't seen yet how fast it could be ...\n",
    "\n",
    "If you look at the GPU utilization (e.g. with `nvidia-smi`) while this is running you will see it is rather low. That's because RNNs are inherently sequential - we can't process the different steps of a sequence in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60929c-5329-47f3-a05b-0ee3211a3dce",
   "metadata": {},
   "source": [
    "But what we can do is process each step of the sequence in parallel across all instances of a batch!\n",
    "\n",
    "Keras will do this if we provide batches that are Tensors of fixed length.\n",
    "\n",
    "To try this out, let's enlarge the sequences to a fixed length and fill missing values with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107fe03-6238-4072-82e7-b15cff1aa352",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = tf.keras.preprocessing.sequence.pad_sequences(arrays, padding=\"post\", dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15b5d1-9ab8-4112-a279-668fc3aaa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b3a21-2be6-4be6-bf33-40727c5daae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b65f1-f9f2-4784-9ebf-e240441e5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(padded, batch_size=256, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902695ca-662b-422e-b7ee-e66aebc64a4d",
   "metadata": {},
   "source": [
    "That should have been **much** faster.\n",
    "\n",
    "But now the model also processed the 0-padded values. We can see that e.g. the first output is different than what we expect from passing in the first sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774e47d-6ada-463a-869e-d1523cc96306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(arrays[0][np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6d477-02a7-48c5-8200-6016bfa1ba04",
   "metadata": {},
   "source": [
    "In keras we can solve this by a `Masking` layer - subsequent RNN layers will respect this and only process non-masked inputs.\n",
    "\n",
    "For more info, see https://keras.io/guides/understanding_masking_and_padding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5441255-387d-49e8-bd51-11b2a2b1c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.0),\n",
    "    tf.keras.layers.LSTM(128, input_shape=(None, 3), return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc14e8d-68bb-4950-a861-ca44604b6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_model.build(input_shape=(None, None, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d0f6b-cb1d-4a7f-8e94-0c7b944371b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights such that we can compare the outputs of both models\n",
    "masked_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22854be-2734-45e1-a5b0-6e08ac3d8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_model.predict(padded, batch_size=256, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113bf8b-3af8-4641-b620-25df6046e0a9",
   "metadata": {},
   "source": [
    "This time the output is compatible with the one-by-one processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
